{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "import gensim\n",
    "from sklearn import svm, metrics\n",
    "import numpy as np\n",
    "from gensim.models import doc2vec\n",
    "from collections import namedtuple\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16134\n"
     ]
    }
   ],
   "source": [
    "#training dataset\n",
    "df = pd.read_csv('P1_training_set.csv')\n",
    "e1 = df['Event 1']\n",
    "e2 = df['Event 2']\n",
    "label = df['Label']\n",
    "#print(e1.size*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = e1.tolist()\n",
    "doc2 = e2.tolist()\n",
    "doc = doc1+doc2\n",
    "#print(doc)\n",
    "#print(len(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(doc1)\n",
    "\n",
    "docs = []\n",
    "analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')\n",
    "for i, text in enumerate(doc):\n",
    "    words = text.lower().split()\n",
    "    tags = [i]\n",
    "    docs.append(analyzedDocument(words, tags))\n",
    "    #print(words,tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spradha5\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:574: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "2019-09-14 14:23:45,957 : INFO : collecting all words and their counts\n",
      "2019-09-14 14:23:45,958 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2019-09-14 14:23:45,988 : INFO : PROGRESS: at example #10000, processed 55550 words (1908200/s), 6036 word types, 10000 tags\n",
      "2019-09-14 14:23:46,007 : INFO : collected 7539 word types and 16134 unique tags from a corpus of 16134 examples and 89375 words\n",
      "2019-09-14 14:23:46,008 : INFO : Loading a fresh vocabulary\n",
      "2019-09-14 14:23:46,022 : INFO : effective_min_count=1 retains 7539 unique words (100% of original 7539, drops 0)\n",
      "2019-09-14 14:23:46,023 : INFO : effective_min_count=1 leaves 89375 word corpus (100% of original 89375, drops 0)\n",
      "2019-09-14 14:23:46,050 : INFO : deleting the raw counts dictionary of 7539 items\n",
      "2019-09-14 14:23:46,050 : INFO : sample=0.001 downsamples 44 most-common words\n",
      "2019-09-14 14:23:46,051 : INFO : downsampling leaves estimated 60306 word corpus (67.5% of prior 89375)\n",
      "2019-09-14 14:23:46,069 : INFO : estimated required memory for 7539 words and 100 dimensions: 16254300 bytes\n",
      "2019-09-14 14:23:46,070 : INFO : resetting layer weights\n",
      "2019-09-14 14:23:46,360 : INFO : training model with 4 workers on 7539 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=300\n",
      "2019-09-14 14:23:46,940 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-09-14 14:23:46,943 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-09-14 14:23:46,947 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-09-14 14:23:46,980 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-09-14 14:23:46,981 : INFO : EPOCH - 1 : training on 89375 raw words (76403 effective words) took 0.6s, 124791 effective words/s\n",
      "2019-09-14 14:23:47,564 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-09-14 14:23:47,568 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-09-14 14:23:47,573 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-09-14 14:23:47,606 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-09-14 14:23:47,607 : INFO : EPOCH - 2 : training on 89375 raw words (76489 effective words) took 0.6s, 123927 effective words/s\n",
      "2019-09-14 14:23:48,180 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-09-14 14:23:48,188 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-09-14 14:23:48,192 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-09-14 14:23:48,228 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-09-14 14:23:48,229 : INFO : EPOCH - 3 : training on 89375 raw words (76550 effective words) took 0.6s, 124908 effective words/s\n",
      "2019-09-14 14:23:48,820 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-09-14 14:23:48,826 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-09-14 14:23:48,834 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-09-14 14:23:48,864 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-09-14 14:23:48,865 : INFO : EPOCH - 4 : training on 89375 raw words (76297 effective words) took 0.6s, 121902 effective words/s\n",
      "2019-09-14 14:23:49,441 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-09-14 14:23:49,455 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-09-14 14:23:49,457 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-09-14 14:23:49,496 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-09-14 14:23:49,497 : INFO : EPOCH - 5 : training on 89375 raw words (76639 effective words) took 0.6s, 124373 effective words/s\n",
      "2019-09-14 14:23:49,497 : INFO : training on a 446875 raw words (382378 effective words) took 3.1s, 121909 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model = doc2vec.Doc2Vec(docs, size = 100, window = 300, min_count = 1, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training vectors\n",
    "xtrain = []\n",
    "ytrain = []\n",
    "\n",
    "for i in range(e1.size):\n",
    "    j=i+e1.size\n",
    "    lab = label.loc[i]\n",
    "    vec1 = (model.docvecs[i]).tolist()\n",
    "    vec2 = (model.docvecs[j]).tolist()\n",
    "    vec = vec1+vec2\n",
    "    \n",
    "    xtrain.append(vec)\n",
    "    ytrain.append(lab)\n",
    "    \n",
    "xtrain = np.array(xtrain).astype(np.float32)\n",
    "ytrain = np.array(ytrain).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=300,\n",
       "                       n_jobs=-1, oob_score=False, random_state=None, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_jobs=-1, n_estimators=300, max_features='auto')\n",
    "rfc.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing dataset\n",
    "df2 = pd.read_csv('P1_testing_set.csv')\n",
    "e1t = df2['Event 1']\n",
    "e2t = df2['Event 2']\n",
    "labelt = df2['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1t = e1t.tolist()\n",
    "doc2t = e2t.tolist()\n",
    "doct = doc1t+doc2t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "docst = []\n",
    "analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')\n",
    "for i, text in enumerate(doct):\n",
    "    words = text.lower().split()\n",
    "    tags = [i]\n",
    "    docst.append(analyzedDocument(words, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spradha5\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:574: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "2019-09-14 14:24:02,854 : INFO : collecting all words and their counts\n",
      "2019-09-14 14:24:02,855 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2019-09-14 14:24:02,863 : INFO : collected 2167 word types and 1800 unique tags from a corpus of 1800 examples and 9957 words\n",
      "2019-09-14 14:24:02,864 : INFO : Loading a fresh vocabulary\n",
      "2019-09-14 14:24:02,928 : INFO : effective_min_count=1 retains 2167 unique words (100% of original 2167, drops 0)\n",
      "2019-09-14 14:24:02,929 : INFO : effective_min_count=1 leaves 9957 word corpus (100% of original 9957, drops 0)\n",
      "2019-09-14 14:24:02,937 : INFO : deleting the raw counts dictionary of 2167 items\n",
      "2019-09-14 14:24:02,938 : INFO : sample=0.001 downsamples 44 most-common words\n",
      "2019-09-14 14:24:02,939 : INFO : downsampling leaves estimated 6702 word corpus (67.3% of prior 9957)\n",
      "2019-09-14 14:24:02,946 : INFO : estimated required memory for 2167 words and 100 dimensions: 3537100 bytes\n",
      "2019-09-14 14:24:02,947 : INFO : resetting layer weights\n",
      "2019-09-14 14:24:03,000 : INFO : training model with 4 workers on 2167 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=300\n",
      "2019-09-14 14:24:03,007 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-09-14 14:24:03,008 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-09-14 14:24:03,009 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-09-14 14:24:03,045 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-09-14 14:24:03,046 : INFO : EPOCH - 1 : training on 9957 raw words (8494 effective words) took 0.0s, 205653 effective words/s\n",
      "2019-09-14 14:24:03,053 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-09-14 14:24:03,054 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-09-14 14:24:03,055 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-09-14 14:24:03,116 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-09-14 14:24:03,119 : INFO : EPOCH - 2 : training on 9957 raw words (8500 effective words) took 0.1s, 129443 effective words/s\n",
      "2019-09-14 14:24:03,129 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-09-14 14:24:03,130 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-09-14 14:24:03,131 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-09-14 14:24:03,176 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-09-14 14:24:03,178 : INFO : EPOCH - 3 : training on 9957 raw words (8481 effective words) took 0.0s, 171910 effective words/s\n",
      "2019-09-14 14:24:03,184 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-09-14 14:24:03,186 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-09-14 14:24:03,186 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-09-14 14:24:03,228 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-09-14 14:24:03,229 : INFO : EPOCH - 4 : training on 9957 raw words (8518 effective words) took 0.0s, 185869 effective words/s\n",
      "2019-09-14 14:24:03,235 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-09-14 14:24:03,236 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-09-14 14:24:03,237 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-09-14 14:24:03,275 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-09-14 14:24:03,276 : INFO : EPOCH - 5 : training on 9957 raw words (8486 effective words) took 0.0s, 201826 effective words/s\n",
      "2019-09-14 14:24:03,277 : INFO : training on a 49785 raw words (42479 effective words) took 0.3s, 153374 effective words/s\n",
      "2019-09-14 14:24:03,278 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "modelt = doc2vec.Doc2Vec(docst, size = 100, window = 300, min_count = 1, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing vectors\n",
    "xtest = []\n",
    "ytest = []\n",
    "\n",
    "for i in range(e1t.size):\n",
    "    j=i+e1t.size\n",
    "    lab = labelt.loc[i]\n",
    "    vec1 = (modelt.docvecs[i]).tolist()\n",
    "    vec2 = (modelt.docvecs[j]).tolist()\n",
    "    vec = vec1+vec2\n",
    "    \n",
    "    xtest.append(vec)\n",
    "    ytest.append(lab)\n",
    "    \n",
    "xtest = np.array(xtest).astype(np.float32)\n",
    "ytest = np.array(ytest).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = rfc.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Confusion Matrix: \n",
      " [[  0 168  19]\n",
      " [  0 341  40]\n",
      " [  0 304  28]]\n",
      "\n",
      " Accuracy:  41.0\n",
      "\n",
      " Precision, Recall, F1-score: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       187\n",
      "         1.0       0.42      0.90      0.57       381\n",
      "         2.0       0.32      0.08      0.13       332\n",
      "\n",
      "    accuracy                           0.41       900\n",
      "   macro avg       0.25      0.33      0.23       900\n",
      "weighted avg       0.30      0.41      0.29       900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Performance Metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('\\n Confusion Matrix: \\n',confusion_matrix(ytest,ypred))\n",
    "print('\\n Accuracy: ', accuracy_score(ytest,ypred)*100)\n",
    "print('\\n Precision, Recall, F1-score: \\n',classification_report(ytest,ypred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
